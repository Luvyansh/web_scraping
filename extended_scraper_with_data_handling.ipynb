{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e5273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def parse_html_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "    \n",
    "    tables = soup.find_all(\"table\")\n",
    "    records = []\n",
    "\n",
    "    for table in tables:\n",
    "        business_name = table.find(\"th\").text.strip()\n",
    "        rows = table.find_all(\"tr\")[1:]\n",
    "        data = {'business_name': business_name}\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            for i in range(0, len(cells), 2):\n",
    "                key = cells[i].text.strip().lower()\n",
    "                value = cells[i+1].text.strip() if i+1 < len(cells) else None\n",
    "                data[key] = value\n",
    "        records.append(data)\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "html_dir = \".\"\n",
    "html_files = [f for f in os.listdir(html_dir) if f.endswith(\".html\")]\n",
    "all_businesses_df = pd.concat([parse_html_file(os.path.join(html_dir, file)) for file in html_files], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some reusable Python functions for the following:\n",
    "# 1. Handling Parquet files\n",
    "# 2. Reading and summarizing PDF files\n",
    "# 3. Connecting to and querying SQLite databases\n",
    "# 4. Creating data visualizations from DataFrames\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import PyPDF2\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "\n",
    "# Function to read and display Parquet file data\n",
    "def read_parquet_file(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    print(\"Parquet File Loaded Successfully\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Head:\")\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "# Function to visualize basic statistics of a DataFrame\n",
    "def visualize_dataframe(df, numerical_only=True):\n",
    "    if numerical_only:\n",
    "        df = df.select_dtypes(include='number')\n",
    "\n",
    "    sns.set(style='whitegrid')\n",
    "    for column in df.columns:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(df[column].dropna(), kde=True, bins=30)\n",
    "        plt.title(f'Distribution of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Function to read and extract text from a PDF file\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or ''\n",
    "    return text.strip()\n",
    "\n",
    "# Function to connect to a SQLite DB and run a sample query\n",
    "def query_sqlite_db(db_path, query=\"SELECT name FROM sqlite_master WHERE type='table';\"):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping and Data Cleaning from Local HTML Files\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. Define a function to extract business data from a single HTML file\n",
    "def extract_business_data_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "\n",
    "    records = []\n",
    "    for table in tables:\n",
    "        business = {}\n",
    "        header = table.find('th')\n",
    "        if header:\n",
    "            business['business_name'] = header.text.strip()\n",
    "        rows = table.find_all('tr')[1:]\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            for i in range(0, len(cells), 2):\n",
    "                key = cells[i].text.strip()\n",
    "                value = cells[i + 1].text.strip()\n",
    "                business[key] = value\n",
    "        records.append(business)\n",
    "\n",
    "    return records\n",
    "\n",
    "# 2. Aggregate data from all HTML files in the current directory\n",
    "html_data = []\n",
    "html_files = [f for f in os.listdir() if f.endswith('.html') and 'biz' in f]\n",
    "\n",
    "for filename in html_files:\n",
    "    with open(filename, encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        html_data.extend(extract_business_data_from_html(content))\n",
    "\n",
    "# 3. Create DataFrame from the scraped data\n",
    "df = pd.DataFrame(html_data)\n",
    "\n",
    "# 4. Clean the DataFrame\n",
    "# Convert \"null\" strings to actual NaNs, remove duplicates\n",
    "clean_df = df.replace({'null': pd.NA}).drop_duplicates()\n",
    "\n",
    "# Convert data types where applicable\n",
    "cols_to_convert = ['business_id', 'latitude', 'longitude', 'business_certificate', 'owner_zip']\n",
    "for col in cols_to_convert:\n",
    "    clean_df[col] = pd.to_numeric(clean_df[col], errors='coerce')\n",
    "\n",
    "# 5. Show data summary and sample\n",
    "print(\"\\n--- Data Overview ---\")\n",
    "print(clean_df.info())\n",
    "print(\"\\n--- Sample Data ---\")\n",
    "print(clean_df.head())\n",
    "\n",
    "# Optional: Save to CSV for analysis or ML tasks\n",
    "clean_df.to_csv('business_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d67ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.replace({'null': pd.NA, '': pd.NA})\n",
    "    df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "    \n",
    "    for col in ['latitude', 'longitude']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "cleaned_df = clean_data(all_businesses_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d38387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_tax_code_distribution(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(y=\"tax_code\", data=df, order=df['tax_code'].value_counts().index)\n",
    "    plt.title(\"Distribution of Business Tax Codes\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(\"Tax Code\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_business_locations(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    valid_coords = df.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "    plt.scatter(valid_coords[\"longitude\"], valid_coords[\"latitude\"], alpha=0.6)\n",
    "    plt.title(\"Business Locations (Lat/Lon)\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed3787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "def read_parquet_file(filepath):\n",
    "    table = pq.read_table(filepath)\n",
    "    return table.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f269b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(filepath):\n",
    "    with open(filepath, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e75add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def load_sqlite_db(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df_list = {}\n",
    "    for table_name in pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)[\"name\"]:\n",
    "        df_list[table_name] = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    return df_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_cleaned_data(df, filename=\"cleaned_businesses.csv\"):\n",
    "    df.to_csv(filename, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language": "python",
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
